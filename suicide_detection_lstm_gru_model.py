# -*- coding: utf-8 -*-
"""Suicide Detection LSTM/GRU Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fNHFvnhig8hmHNjl-dWo06DgtP41gqth
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import nltk
from nltk.corpus import stopwords
import re
nltk.download('stopwords')

## df = pd.read_csv("/content/drive/MyDrive/UoB Dissertation Folder/500_Reddit_users_posts_labels (1).csv")

###df = pd.read_excel("/content/drive/MyDrive/UoB Dissertation Folder/500_Reddit_users_posts_labels (1).xlsx")

df = pd.read_csv("/content/drive/MyDrive/UoB Dissertation Folder/500_Reddit_users_posts_labels (2).csv")

df.head()

df["Label"].value_counts()

for Post in df['Post'][:5]:
  print("\n\n", Post)

stop_words = set(stopwords.words('english'))

def text_cleaner(Post):
  #converting to lowercase
  newString = Post.lower()
  #removing links
  newString = re.sub(r'(https|http)?:\/\/(\w|\.|\/|\?|\=|\&|\%)*\b', '', newString)
  #removing text inside ()
  newString = re.sub(r'\([^)]*\)', '', newString)
  #removing text inside []
  newString = re.sub(r'\{[^)]*\}', '', newString)
  #fetching alhpabetic charaters
  newString = re.sub("[^a-zA-Z]", " ", newString)
  #removing stop words
  tokens = [w for w in newString.split() if not w in stop_words]
  long_words=[]
  for i in tokens:
    if len(i)>=4:
      long_words.append(i)
  return (" ".join(long_words)).strip()

text1 = df['Post'][2]
clean_text1 = text_cleaner(text1)
print("Before cleaning:\n",text1)
print("After cleaning:\n",clean_text1)

text1 = df['Post'][2]
clean_text1 = text_cleaner(text1)
print("Before cleaning:\n", text1)
print("After cleaning:\n", clean_text1)

cleaned_text=[]
for i in df['Post']:
    cleaned_text.append(text_cleaner(i))

df["Label"]

## df.isnull().sum()

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y=le.fit_transform(df['Label'])

y

from sklearn.model_selection import train_test_split
x_tr,x_val,y_tr,y_val=train_test_split(cleaned_text,y,test_size=0.3,random_state=0,shuffle=True)

import matplotlib.pyplot as plt

text_word_count = []

#populate the lists with sentence lengths
for i in cleaned_text:
      text_word_count.append(len(i.split()))

length_df = pd.DataFrame({'text':text_word_count})

length_df.hist(bins = 100, range=(0,50000))
plt.show()

max_len=200

from keras.preprocessing.text import Tokenizer 
from keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer()
#creating index for a wor
tokenizer.fit_on_texts(list(x_tr))

#converting word seq to integer seq
x_tr    =   tokenizer.texts_to_sequences(x_tr) 
x_val   =   tokenizer.texts_to_sequences(x_val)

#padding up with zero 
x_tr    =   pad_sequences(x_tr,  maxlen=max_len, padding='post')
x_val   =   pad_sequences(x_val, maxlen=max_len, padding='post')

vocabulary   =  len(tokenizer.word_index) +1
print("Vocabulary size:",vocabulary)

#sequence encoding and padding
x_tr[150]

#showing word index 
tokenizer.word_index

from keras.utils.np_utils import to_categorical
y_tr=to_categorical(y_tr,num_classes=9)
y_val=to_categorical(y_val,num_classes=9)

from keras.models import Sequential
from keras.layers import LSTM, Dense, Embedding, GRU
from keras.callbacks import EarlyStopping, ModelCheckpoint
import keras.backend as K
K.clear_session()

# LSTM model
model=Sequential()
model.add(Embedding(vocabulary,100,input_length=max_len,trainable=True, mask_zero=True)) 
model.add(LSTM(300,dropout=0.1, recurrent_dropout=0.2)) 
model.add(Dense(64,activation='relu')) 
model.add(Dense(9,activation='softmax')) 
print(model.summary())

model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=["acc"])

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)  
mc=ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', save_best_only=True,verbose=1)

history = model.fit(np.array(x_tr),np.array(y_tr),batch_size=700,epochs=100,validation_data=(np.array(x_val),np.array(y_val)),verbose=1,callbacks=[es,mc])

score, acc = model.evaluate(x_val, y_val,verbose=2, batch_size= 256)
print('test accuracy:', acc)

from matplotlib import pyplot
pyplot.plot(history.history['loss'], label='train')
pyplot.plot(history.history['val_loss'], label='test')
pyplot.legend()
pyplot.show()

from keras.models import load_model
model = load_model('best_model.h5')

print('test accuracy:', acc)

predictions = (model.predict(x_val) > 0.5).astype("int32")

print("prediction")